import argparse
import jsonlines as jsl
from typing import List, Optional, Dict, Any, Tuple
import os
import random
from tqdm import tqdm
from scipy.stats import pearsonr
import sys

sys.path.append("/media/data/2/yx/model_toxic/my_project/models/others")
from modeling import GPT2_or_Llama_Wrapper
from ranker import Ranker_Base
from base import Base_M
from constant import PATTERNS, ContextLevel_PATTERNS, PATTERNS_threshold, ContextLevel_PATTERNS_threshold

def chunks(lst: List, n: int):
    """Yield successive n-sized chunks from lst."""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]


def build_input_text(pattern: str, text: str, replace_newlines: bool = True):
    assert '<INPUT>' in pattern
    if replace_newlines:
        text = text.replace('\n', ' ')
    return pattern.replace('<INPUT>', text)

def buid_contextlevel_input_text(pattern: str, question: str, response: str, replace_newlines: bool = True):
    assert '{question}' in pattern
    assert '{response}' in pattern
    if replace_newlines:
        question = question.replace('\n', ' ')
        response = response.replace('\n', ' ')
    return pattern.replace('{question}', question).replace('{response}', response)

class ModelOutput:
    """This class represents a piece of text generated by a language model, as well as corresponding attribute scores"""

    TEXT_REPR_MAX_LEN = 50

    def __init__(self, text: str, scores: Dict[str, float]):
        """
        :param text: the generated text
        :param scores: the attribute scores
        """
        self.text = text
        self.scores = scores

class DataFormat:
    """This class represents a piece of text generated by a language model, as well as corresponding attribute scores"""
    def __init__(self, question: str, text: str):

        self.question = question
        self.text = text


def load_data_format(prompts: list, continutions: list) -> List[DataFormat]:
    examples = []
    for question, text in zip(prompts, continutions):
        examples.append(DataFormat(question, text))
    return examples


class Self_GPT_or_Llama_Score_Rank(Base_M):
    def __init__(self, config):
        super(Self_GPT_or_Llama_Score_Rank, self).__init__(config)

        self.config = config
        self.wrapper = GPT2_or_Llama_Wrapper(cache_path=self.config.model_path, model_name=None)
        self.batch_size = 32
        self.output_choices = ['yes', 'no']
        self.ranker = Ranker_Base(config)  # todo: 所有的选择方法集成在这里
        self.prompt_toxic_is_true = True if config.prompt_type == 'toxic' else False

    # 评分每个生成的毒性
    def run_toxicity_evaluation_by_self(self, prompts: List[str] = None, continuations: List[str] = None,
                                        batch_size: int = 32, attribute: str = 'toxicity', data_is_ContexLevel: bool = False):
        if data_is_ContexLevel:
            pattern = ContextLevel_PATTERNS[attribute]
        else:
            pattern = PATTERNS[attribute]
        examples = load_data_format(prompts, continuations)

        example_iterator = tqdm(list(chunks(examples, batch_size)), desc="Example batches")

        scores = []
        for example_batch in example_iterator:
            if data_is_ContexLevel:
                input_texts = [buid_contextlevel_input_text(pattern, example.question, example.text) for example in
                               example_batch]
            else:
                input_texts = [build_input_text(pattern, example.text) for example in example_batch]

            token_probability_distribution = self.wrapper.get_token_probability_distribution(input_texts,
                                                                                             output_choices=self.output_choices)  # todo: 获取结果是”yes“, "no"的不同概率
            for idx, example in enumerate(example_batch):
                # token_probability_distribution[idx] is of the form [("Yes", p_yes), ("No", p_no)], so we obtain the probability of the input
                # exhibiting the considered attribute by looking at index (0,1)
                scores.append(token_probability_distribution[idx][0][1])  # todo: 获取是 ”yes“的概率
        return scores

    # 分为 continuations_nontoxic 和 continuations_toxic 两个部分
    def self_filter(self, results=None, scores=None, other_scores=None, attribute: str = 'toxicity', other_attributes=None, data_is_ContexLevel: bool = False):
        model_name = self.config.model_path.split('/')[-1]
        if data_is_ContexLevel:
            threshold = ContextLevel_PATTERNS_threshold[model_name][attribute]
        else:
            threshold = PATTERNS_threshold[model_name]['toxic'][attribute] if self.prompt_toxic_is_true \
                        else PATTERNS_threshold[model_name]['nontoxic'][attribute]

        sample_dict = dict(
            prompt=dict(),
            continuations_toxic=[],
            continuations_nontoxic=[]
        )
        filter_results = []
        for i, s in tqdm(enumerate(scores), desc='self_filter samples'):
            sample_dict['prompt'] = results[i // self.config.num_k_samples]['prompt']
            text = results[i // self.config.num_k_samples]['continuations'][i % self.config.num_k_samples]['text']
            sample = dict()
            sample['text'] = text
            sample[attribute] = s
            if other_scores is None:
                pass
            else:
                for attr in other_attributes:
                    sample[attr] = other_scores[attr][i]

            if s >= threshold:
                sample_dict['continuations_toxic'].append(sample)
            elif s < threshold:
                sample_dict['continuations_nontoxic'].append(sample)

            if (i + 1) % self.config.num_k_samples == 0:
                filter_results.append(sample_dict)
                sample_dict = dict(
                    prompt=dict(),
                    continuations_toxic=[],
                    continuations_nontoxic=[]
                )
        return filter_results

    # 选择哪些示例
    def rank_and_select(self, filter_results, p_num, main_attribute=None):

        if self.config.select_type == 'toxicity_topk':
            select_function = self.ranker.toxic_topk_nontoxic_bottom_select
            prompt_operator = self.prefix_build_with_global

        elif self.config.select_type == 'toxicity_top1+subtoxicity':
            select_function = self.ranker.toxic_subtopk_nontoxic_bottom
            prompt_operator = self.prefix_build_with_global

        # ---- 最终使用的那个 毒性输出/ 非毒性输出 位置： 只使用毒性最大的多个样本 和 毒性最小的那个样本的位置----
        num_toxic_total = 0
        num_nontoxic_total = 0
        num_total = len(filter_results)
        max_toxic_index_list = [dict(ids=None) for _ in range(num_total)]
        min_nontoxic_index_list = [dict(ids=None) for _ in range(num_total)]

        for ids, samp in enumerate(filter_results):
            toxicity_list = []
            nontoxicity_list = []
            for k, ss in enumerate(samp['continuations_toxic']):
                toxicity_list.append(ss[main_attribute])
            for j, s in enumerate(samp['continuations_nontoxic']):
                nontoxicity_list.append(s[main_attribute])

            combine_list = samp['continuations_toxic'] + samp['continuations_nontoxic']

            max_indexs = select_function(toxicity_list=toxicity_list, p_num=p_num, direct='max', combine_list=combine_list)
            min_indexs = select_function(nontoxicity_list=nontoxicity_list, p_num=p_num, direct='min', combine_list=combine_list)
            max_toxic_index_list[ids]['ids'] = max_indexs
            min_nontoxic_index_list[ids]['ids'] = min_indexs

        print(f'----prompt type: {self.config.prompt_type}----')
        print(
            f'total num: {num_total} ||  total toxic num: {num_toxic_total}  || total nontoxic num: {num_nontoxic_total}')

        # 构建 incontext learning before prompt
        return prompt_operator(filter_results, max_toxic_index_list, min_nontoxic_index_list, p_num, pad_neg_sample=self.config.pad_neg_sample if 'pad_neg_sample' in self.config else True)

    def delete(self):
        self.wrapper.delete()

